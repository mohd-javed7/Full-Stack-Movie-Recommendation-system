{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: To make a recommender system that takes input and recommends 5 similair movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('tmdb_5000_movies.csv')\n",
    "credits = pd.read_csv('tmdb_5000_credits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.merge(credits,on='title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columns to keep\n",
    "- genres\n",
    "- id\n",
    "- keywords\n",
    "- title\n",
    "- overview\n",
    "- tagline\n",
    "- cast \n",
    "- crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies[['id','genres','keywords','overview','title','tagline','cast','crew']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "genres        0\n",
       "keywords      0\n",
       "overview      3\n",
       "title         0\n",
       "tagline     844\n",
       "cast          0\n",
       "crew          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.dropna(inplace=True) # removing the movies with missing overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.duplicated().sum() # no duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we make the genres,keywords,cast,crew into list as right now they are in different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"name\": \"Fantasy\"}, {\"id\": 878, \"name\": \"Science Fiction\"}]'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies['genres'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def convert(obj):\n",
    "    L = []\n",
    "    for i in ast.literal_eval(obj):\n",
    "        L.append(i['name'])\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Action', 'Adventure', 'Fantasy', 'Science Fiction']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert('[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"name\": \"Fantasy\"}, {\"id\": 878, \"name\": \"Science Fiction\"}]')\n",
    "# the genres has list inside string so we first need to convert them back to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for every movie\n",
    "\n",
    "movies['genres'] = movies['genres'].apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['genres']\n",
    "# this is giving result as expected.\n",
    "# now the same thing with keywords.\n",
    "movies['keywords'] = movies['keywords'].apply(convert)\n",
    "# now the cast - taking top 5 actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def convert(obj):\n",
    "    L = []\n",
    "    for i in ast.literal_eval(obj):\n",
    "           if(i['job']=='Director'):\n",
    "            L.append(i['name'])\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['cast'] = movies['cast'].apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['crew'] = movies['crew'].apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['tagline']=movies['tagline'].apply(lambda x: x.lower().split())\n",
    "movies['overview']=movies['overview'].apply(lambda x: x.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse(para):\n",
    "    return [i.replace(' ','').lower() for i in para]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['genres'] = movies['genres'].apply(collapse)\n",
    "movies['keywords'] = movies['keywords'].apply(collapse)\n",
    "movies['cast'] = movies['cast'].apply(collapse)\n",
    "movies['crew'] = movies['crew'].apply(collapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['tags'] = movies['overview']+movies['genres']+movies['keywords']+movies['cast']+movies['crew']+movies['tagline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [in, the, 22nd, century,, a, paraplegic, marin...\n",
       "Name: tags, dtype: object"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head(1)['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13284\\3165177407.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MOVIES['tags'] = MOVIES['tags'].apply(lambda x: ' '.join(re.sub(r'[^\\w\\s]', '', word) for word in x))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "MOVIES['tags'] = MOVIES['tags'].apply(lambda x: ' '.join(re.sub(r'[^\\w\\s]', '', word) for word in x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    y=[]\n",
    "    for i in text.split():\n",
    "        y.append(ps.stem(i))\n",
    "\n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13284\\3932991792.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MOVIES['tags'] = MOVIES['tags'].apply(stem)\n"
     ]
    }
   ],
   "source": [
    "MOVIES['tags'] = MOVIES['tags'].apply(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing has been completed.\n",
    "- Now going to convert the movies into vectors and use cosine similairity to get the how close the movies are to each other and recommend movies closest to them.\n",
    "- How vectorization is going to happen first find the most common words in the total movies and for me here i am choosing total 5000 words for common words.\n",
    "- then out of those 5000 common words every movie will be checked kind of like if the contain those common words are in that movie and this will be done for all the 5000 words with the movie tags and this will vectors.\n",
    "- next what will happen is after the movies are converted into vectors, now I will use cosine similarity (A.B/|A|x|B|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.7/8.7 MB 89.1 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   ---------------------------------------- 2/2 [scikit-learn]\n",
      "\n",
      "Successfully installed scikit-learn-1.7.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfid = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "vectors = tfid.fit_transform(MOVIES['tags']).toarray()\n",
    "similarity = cosine_similarity(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[201]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mMOVIES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMOVIES\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m==\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAvatar\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5401\u001b[39m, in \u001b[36mIndex.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   5398\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[32m   5399\u001b[39m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[32m   5400\u001b[39m     key = com.cast_scalar_indexer(key)\n\u001b[32m-> \u001b[39m\u001b[32m5401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   5404\u001b[39m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[32m   5405\u001b[39m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[32m   5406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_slice(key)\n",
      "\u001b[31mIndexError\u001b[39m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "MOVIES[MOVIES['tags']==\"Avatar\"].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movie):\n",
    "    movie_index = MOVIES[MOVIES['title']==movie].index[0]\n",
    "    distance = similarity[movie_index]\n",
    "    movie_list = sorted(list(enumerate(distance)),reverse=True,key=lambda x: x[1])\n",
    "\n",
    "    for i in movie_list[1:6]:\n",
    "        print(MOVIES.iloc[i[0]].title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malone\n",
      "Slow Burn\n",
      "Patriot Games\n",
      "Nixon\n",
      "Jason Bourne\n"
     ]
    }
   ],
   "source": [
    "recommend(\"Hero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIES.to_csv('cleaned_movies.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The MOVIES dataset is good but it only has hollywood movies so right now takin one bollywood movies dataset and after preprocessing bollywood dataset will concatinate with MOVIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1284, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bollywood = pd.read_csv('BollywoodMovieDetail.csv')\n",
    "bollywood.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Going to keep all the same columns as the previous movie Dataset. One flaw that this movie has is it doesn't have overview so i have to get the overview from api if i can find or else have to deal with it without api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1284 entries, 0 to 1283\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   imdbId       1284 non-null   object \n",
      " 1   title        1284 non-null   object \n",
      " 2   releaseYear  1284 non-null   int64  \n",
      " 3   releaseDate  1231 non-null   object \n",
      " 4   genre        1282 non-null   object \n",
      " 5   writers      1165 non-null   object \n",
      " 6   actors       1281 non-null   object \n",
      " 7   directors    1280 non-null   object \n",
      " 8   sequel       1281 non-null   float64\n",
      " 9   hitFlop      1284 non-null   int64  \n",
      "dtypes: float64(1), int64(2), object(7)\n",
      "memory usage: 100.4+ KB\n"
     ]
    }
   ],
   "source": [
    "bollywood.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollywood = bollywood[['imdbId','title','genre','actors','directors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollywood.isnull().sum()\n",
    "bollywood.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now cleaning the columns for commas or pipe [|,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollywood['title'] = bollywood['title'].str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollywood['genre'] = bollywood['genre'].str.replace(r'[|,]', ' ', regex=True)\n",
    "bollywood['actors'] = bollywood['actors'].str.replace(r'[|,]', ' ', regex=True)\n",
    "bollywood['directors'] = bollywood['directors'].str.replace(r'[|,]', ' ', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollywood['genre'] = bollywood['genre'].apply(collapse)\n",
    "bollywood['actors'] = bollywood['actors'].apply(collapse)\n",
    "bollywood['directors'] = bollywood['directors'].apply(collapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbId</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>actors</th>\n",
       "      <th>directors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0118578</td>\n",
       "      <td>albela</td>\n",
       "      <td>romance</td>\n",
       "      <td>govinda aishwaryaraibachchan jackieshroff namr...</td>\n",
       "      <td>deepaksareen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0169102</td>\n",
       "      <td>lagaan: once upon a time in india</td>\n",
       "      <td>adventure drama musical</td>\n",
       "      <td>aamirkhan gracysingh rachelshelley paulblackth...</td>\n",
       "      <td>ashutoshgowariker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0187279</td>\n",
       "      <td>meri biwi ka jawab nahin</td>\n",
       "      <td>action comedy</td>\n",
       "      <td>akshaykumar sridevi gulshangrover laxmikantberde</td>\n",
       "      <td>pankajparashar s.m.iqbal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0222024</td>\n",
       "      <td>hum tumhare hain sanam</td>\n",
       "      <td>drama romance</td>\n",
       "      <td>shahrukhkhan madhuridixit salmankhan atulagnih...</td>\n",
       "      <td>k.s.adiyaman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0227194</td>\n",
       "      <td>one 2 ka 4</td>\n",
       "      <td>action comedy drama</td>\n",
       "      <td>shahrukhkhan juhichawla jackieshroff nirmalpandey</td>\n",
       "      <td>shashilalk.nair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      imdbId                              title                    genre  \\\n",
       "0  tt0118578                             albela                  romance   \n",
       "1  tt0169102  lagaan: once upon a time in india  adventure drama musical   \n",
       "2  tt0187279           meri biwi ka jawab nahin            action comedy   \n",
       "3  tt0222024             hum tumhare hain sanam            drama romance   \n",
       "4  tt0227194                         one 2 ka 4      action comedy drama   \n",
       "\n",
       "                                              actors                 directors  \n",
       "0  govinda aishwaryaraibachchan jackieshroff namr...              deepaksareen  \n",
       "1  aamirkhan gracysingh rachelshelley paulblackth...         ashutoshgowariker  \n",
       "2   akshaykumar sridevi gulshangrover laxmikantberde  pankajparashar s.m.iqbal  \n",
       "3  shahrukhkhan madhuridixit salmankhan atulagnih...              k.s.adiyaman  \n",
       "4  shahrukhkhan juhichawla jackieshroff nirmalpandey           shashilalk.nair  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bollywood.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertNames(name):\n",
    "        name=name.split('|')\n",
    "        clean_name=  []\n",
    "        for i in name:\n",
    "            word = i.strip().replace(' ','').lower()\n",
    "            if word:\n",
    "                  clean_name.append(word)\n",
    "        return \" \".join(clean_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollywood['actors'] = bollywood['actors'].apply(convertNames)\n",
    "bollywood['genre'] = bollywood['genre'].apply(convertNames)\n",
    "bollywood['directors'] = bollywood['directors'].apply(convertNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl (107 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests\n",
      "\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   ---------------------------------------- 5/5 [requests]\n",
      "\n",
      "Successfully installed certifi-2025.10.5 charset_normalizer-3.4.4 idna-3.11 requests-2.32.5 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "API_KEY = \"118d023eb1bdf2ef152b4b5a220eee01\"\n",
    "\n",
    "def get_movie_overview(movie_name):\n",
    "    try:\n",
    "        url = f\"https://api.themoviedb.org/3/search/movie?api_key={API_KEY}&query={movie_name}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        # if results found\n",
    "        if data['results']:\n",
    "            overview = data['results'][0]['overview']\n",
    "            return overview\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching overview for {movie_name}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollywood['overview'] = bollywood['title'].apply(get_movie_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollywood['overview'] = bollywood['overview'].apply(lambda x: \" \".join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollywood['tags'] = bollywood['genre']+' ' +bollywood['actors']+' ' + bollywood['directors']+' ' + bollywood['overview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollywood = bollywood[['imdbId', 'title', 'tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfid = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "vectorsB = tfid.fit_transform(bollywood['tags']).toarray()\n",
    "similarityB = cosine_similarity(vectorsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movie):\n",
    "    movie_index = bollywood[bollywood['title']==movie.lower()].index[0]\n",
    "    distanceB = similarityB[movie_index]\n",
    "    movie_list = sorted(list(enumerate(distanceB)),reverse=True,key=lambda x: x[1])\n",
    "\n",
    "    for i in movie_list[1:6]:\n",
    "        print(bollywood.iloc[i[0]].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zindagi tere naam\n",
      "99\n",
      "grahan\n",
      "shudra the rising\n",
      "lajja\n"
     ]
    }
   ],
   "source": [
    "recommend('Devdas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIES = pd.read_csv('cleaned_movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIES['title'] = MOVIES['title'].str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfid = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "vectors = tfid.fit_transform(MOVIES['tags']).toarray()\n",
    "similarity = cosine_similarity(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movie):\n",
    "    movie_index = MOVIES[MOVIES['title']==movie.lower()].index[0]\n",
    "    distance = similarity[movie_index]\n",
    "    movie_list = sorted(list(enumerate(distance)),reverse=True,key=lambda x: x[1])\n",
    "\n",
    "    for i in movie_list[1:6]:\n",
    "        print(MOVIES.iloc[i[0]].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliens\n",
      "star trek into darkness\n",
      "meet dave\n",
      "apollo 18\n",
      "titan a.e.\n"
     ]
    }
   ],
   "source": [
    "recommend('avatar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_Movies = pd.concat([MOVIES,bollywood],ignore_index=True)\n",
    "final_Movies.drop_duplicates(subset='title', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>imdbId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2223.0</td>\n",
       "      <td>avatar</td>\n",
       "      <td>in the 22nd centuri a parapleg marin is dispat...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2223.0</td>\n",
       "      <td>pirates of the caribbean: at world's end</td>\n",
       "      <td>captain barbossa long believ to be dead ha com...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2223.0</td>\n",
       "      <td>spectre</td>\n",
       "      <td>a cryptic messag from bond past send him on a ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2223.0</td>\n",
       "      <td>the dark knight rises</td>\n",
       "      <td>follow the death of district attorney harvey d...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2223.0</td>\n",
       "      <td>john carter</td>\n",
       "      <td>john carter is a warweari former militari capt...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                     title  \\\n",
       "0  2223.0                                    avatar   \n",
       "1  2223.0  pirates of the caribbean: at world's end   \n",
       "2  2223.0                                   spectre   \n",
       "3  2223.0                     the dark knight rises   \n",
       "4  2223.0                               john carter   \n",
       "\n",
       "                                                tags imdbId  \n",
       "0  in the 22nd centuri a parapleg marin is dispat...    NaN  \n",
       "1  captain barbossa long believ to be dead ha com...    NaN  \n",
       "2  a cryptic messag from bond past send him on a ...    NaN  \n",
       "3  follow the death of district attorney harvey d...    NaN  \n",
       "4  john carter is a warweari former militari capt...    NaN  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_Movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfid = TfidfVectorizer(max_features=8000, stop_words='english')\n",
    "vectors = tfid.fit_transform(final_Movies['tags']).toarray()\n",
    "similarity = cosine_similarity(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movie):\n",
    "    if movie not in final_Movies['title'].str.lower().values:\n",
    "        print(f\"No search results for '{movie}'.\")\n",
    "        return\n",
    "    \n",
    "    movie_index = final_Movies[final_Movies['title']==movie.lower()].index[0]\n",
    "    distance = similarity[movie_index]\n",
    "    movie_list = sorted(list(enumerate(distance)),reverse=True,key=lambda x: x[1])\n",
    "\n",
    "    for i in movie_list[1:6]:\n",
    "        print(final_Movies.iloc[i[0]].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No search results for 'koi mil gaya'.\n"
     ]
    }
   ],
   "source": [
    "recommend('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_Movies.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Movies = pd.read_csv('Main_Movies_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfid = TfidfVectorizer(max_features=8000, stop_words='english')\n",
    "vectors = tfid.fit_transform(Movies['tags']).toarray()\n",
    "similarity = cosine_similarity(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movie):\n",
    "    if movie not in Movies['title'].str.lower().values:\n",
    "        print(f\"No search results for '{movie}'.\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    movie_index = Movies[Movies['title']==movie.lower()].index[0]\n",
    "    distance = similarity[movie_index]\n",
    "    movie_list = sorted(list(enumerate(distance)),reverse=True,key=lambda x: x[1])\n",
    "\n",
    "    for i in movie_list[1:6]:\n",
    "        print(Movies.iloc[i[0]].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meet dave\n",
      "planet 51\n",
      "battlefield earth\n",
      "escape from planet earth\n",
      "titan a.e.\n"
     ]
    }
   ],
   "source": [
    "recommend('home')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"118d023eb1bdf2ef152b4b5a220eee01\"\n",
    "\n",
    "def get_movie_overview(movie_name):\n",
    "    try:\n",
    "        url = f\"https://api.themoviedb.org/3/search/movie?api_key={API_KEY}&query={movie_name}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        # if results found\n",
    "        if data['results']:\n",
    "            overview = data['results'][0]['overview']\n",
    "            return overview\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching overview for {movie_name}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "API_KEY = \"118d023eb1bdf2ef152b4b5a220eee01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script dotenv.exe is installed in 'c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMovie(movie):\n",
    "    try:\n",
    "        url = f\"https://api.themoviedb.org/3/search/movie?api_key={API_KEY}&query={movie}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        if data['results']:\n",
    "            print(f\"{data['results'][0]['title']}\")\n",
    "            movie_id = data['results'][0]['id']\n",
    "            details_url = f\"https://api.themoviedb.org/3/movie/{movie_id}?api_key={API_KEY}&append_to_response=credits,keywords\"\n",
    "            details = requests.get(details_url).json()\n",
    "            return details\n",
    "        else:\n",
    "            return f\"No search results for {movie}\"\n",
    "    except Exception as e:\n",
    "        return (f\"Error finding the {movie}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lagaan: Once Upon a Time in India\n"
     ]
    }
   ],
   "source": [
    "details = getMovie(\"lagaan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'adult': False,\n",
       "  'gender': 2,\n",
       "  'id': 52763,\n",
       "  'known_for_department': 'Acting',\n",
       "  'name': 'Aamir Khan',\n",
       "  'original_name': 'Aamir Khan',\n",
       "  'popularity': 1.2512,\n",
       "  'profile_path': '/iCBtJHaCmdashFEaFOyO0gSteJk.jpg',\n",
       "  'cast_id': 1,\n",
       "  'character': 'Bhuvan',\n",
       "  'credit_id': '52fe47e99251416c750a99a9',\n",
       "  'order': 0},\n",
       " {'adult': False,\n",
       "  'gender': 1,\n",
       "  'id': 85240,\n",
       "  'known_for_department': 'Acting',\n",
       "  'name': 'Gracy Singh',\n",
       "  'original_name': 'Gracy Singh',\n",
       "  'popularity': 1.0324,\n",
       "  'profile_path': '/nJRYXqpctHNuswpesaSm6GFsyX0.jpg',\n",
       "  'cast_id': 2,\n",
       "  'character': 'Gauri',\n",
       "  'credit_id': '52fe47e99251416c750a99ad',\n",
       "  'order': 1},\n",
       " {'adult': False,\n",
       "  'gender': 1,\n",
       "  'id': 80385,\n",
       "  'known_for_department': 'Acting',\n",
       "  'name': 'Rachel Shelley',\n",
       "  'original_name': 'Rachel Shelley',\n",
       "  'popularity': 1.025,\n",
       "  'profile_path': '/AoZAGf20aNUCqh2ujPxfyC1Sk5O.jpg',\n",
       "  'cast_id': 3,\n",
       "  'character': 'Elizabeth Russell',\n",
       "  'credit_id': '52fe47e99251416c750a99b1',\n",
       "  'order': 2},\n",
       " {'adult': False,\n",
       "  'gender': 2,\n",
       "  'id': 43231,\n",
       "  'known_for_department': 'Acting',\n",
       "  'name': 'Paul Blackthorne',\n",
       "  'original_name': 'Paul Blackthorne',\n",
       "  'popularity': 0.5944,\n",
       "  'profile_path': '/3OC1M0rKJO8B3JpwAeavk5EAscl.jpg',\n",
       "  'cast_id': 4,\n",
       "  'character': 'Captain Andrew Russell',\n",
       "  'credit_id': '52fe47e99251416c750a99b5',\n",
       "  'order': 3},\n",
       " {'adult': False,\n",
       "  'gender': 1,\n",
       "  'id': 110708,\n",
       "  'known_for_department': 'Acting',\n",
       "  'name': 'Suhasini Mulay',\n",
       "  'original_name': 'Suhasini Mulay',\n",
       "  'popularity': 0.2343,\n",
       "  'profile_path': '/9mD9ASIlw56LIRIEGSXHmIovu4D.jpg',\n",
       "  'cast_id': 6,\n",
       "  'character': 'Yashoda',\n",
       "  'credit_id': '52fe47e99251416c750a99bf',\n",
       "  'order': 4}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_Details(details):\n",
    "    movie_id = details.get('id',None)\n",
    "    title = details.get('title','').lower()\n",
    "    \n",
    "    overview = details.get('overview', '').lower()\n",
    "    genres = [g['name'].lower().replace(\" \", \"\") for g in details.get('genres', [])]\n",
    "    keywords = [k['name'].lower().replace(\" \", \"\") for k in details.get('keywords', {}).get('keywords', [])]\n",
    "    cast = [c['name'].lower().replace(\" \", \"\") for c in details.get('credits', {}).get('cast', [])[:5]]\n",
    "    crew = [c['name'].lower().replace(\" \", \"\") for c in details.get('credits', {}).get('crew', []) if c.get('job') == 'Director']\n",
    "    tagline = details.get('tagline','').lower()\n",
    "\n",
    "    tags = \" \".join(genres + keywords + cast + crew + [overview]+[tagline])\n",
    "    cleaned = pd.DataFrame([{\n",
    "        'id': movie_id,\n",
    "        'title': title,\n",
    "        'tags': tags\n",
    "    }])\n",
    "\n",
    "\n",
    "    def stem(text):\n",
    "        y=[]\n",
    "        for i in text.split():\n",
    "         y.append(ps.stem(i))\n",
    "\n",
    "        return \" \".join(y)\n",
    "\n",
    "    cleaned['tags'] = cleaned['tags'].apply(stem)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movie):\n",
    "    if movie not in Movies['title'].str.lower().values:\n",
    "        details = getMovie(movie)\n",
    "        if not details:\n",
    "             return\n",
    "        if details:\n",
    "            new_Movie = clean_Details(details)\n",
    "            Movies_dataSet = pd.concat([Movies_dataSet, new_Movie], ignore_index=True)\n",
    "\n",
    "\n",
    "    movie_index = Movies[Movies['title']==movie.lower()].index[0]\n",
    "    distance = similarity[movie_index]\n",
    "    movie_list = sorted(list(enumerate(distance)),reverse=True,key=lambda x: x[1])\n",
    "\n",
    "    for i in movie_list[1:6]:\n",
    "        print(Movies.iloc[i[0]].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Hero 6\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrecommend\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbig hero\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mrecommend\u001b[39m\u001b[34m(movie)\u001b[39m\n\u001b[32m      5\u001b[39m          \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo search results for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmovie\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m          \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m new_Movie = \u001b[43mclean_Details\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetails\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m movie_index = Movies[Movies[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m]==movie.lower()].index[\u001b[32m0\u001b[39m]\n\u001b[32m     11\u001b[39m distance = similarity[movie_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mclean_Details\u001b[39m\u001b[34m(details)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_Details\u001b[39m(details):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mdetails\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mresults\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mKeyError\u001b[39m: 'results'"
     ]
    }
   ],
   "source": [
    "recommend('big hero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollywood = pd.read_csv('cleaned_bollywood.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    y=[]\n",
    "    for i in text.split():\n",
    "        y.append(ps.stem(i))\n",
    "\n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bollywood['tags'] = bollywood['tags'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hollywood = pd.read_csv('cleaned_movies.csv')\n",
    "hollywood['title'] = hollywood['title'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moives_dataSet = pd.concat([hollywood,bollywood],ignore_index=True)\n",
    "Moives_dataSet.drop_duplicates(subset='title', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moives_dataSet.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfid = TfidfVectorizer(max_features=8000, stop_words='english')\n",
    "vectors = tfid.fit_transform(Moives_dataSet['tags']).toarray()\n",
    "similarity = cosine_similarity(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movie):\n",
    "    global Moives_dataSet, similarity  # make sure we can update the dataset\n",
    "    \n",
    "    movie = movie.lower()  \n",
    "    \n",
    "    # Check if movie exists in current dataset\n",
    "    movie_row = Moives_dataSet[Moives_dataSet['title'] == movie]\n",
    "\n",
    "    if movie not in Moives_dataSet['title'].values:\n",
    "        details = getMovie(movie)\n",
    "        if not details:\n",
    "            print(f\"No search results for '{movie}'.\")\n",
    "            return\n",
    "        \n",
    "        new_Movie = clean_Details(details)\n",
    "        # Add and reset index\n",
    "        Moives_dataSet = pd.concat([Moives_dataSet, new_Movie], ignore_index=True)\n",
    "        Moives_dataSet.drop_duplicates(subset='title', inplace=True)\n",
    "        Moives_dataSet.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Recompute similarity for updated dataset\n",
    "        vectors = tfid.fit_transform(Moives_dataSet['tags']).toarray()\n",
    "        similarity = cosine_similarity(vectors)\n",
    "    \n",
    "    # Find index safely\n",
    "    movie_row = Moives_dataSet[Moives_dataSet['title'] == movie]\n",
    "    if movie_row.empty:\n",
    "        print(f\"Movie '{movie}' not found even after adding.\")\n",
    "        return\n",
    "    \n",
    "    movie_index = movie_row.index[0]\n",
    "    distance = similarity[movie_index]\n",
    "    \n",
    "    # Get top 5 recommendations\n",
    "    movie_list = sorted(list(enumerate(distance)), reverse=True, key=lambda x: x[1])[1:6]\n",
    "    \n",
    "    for i in movie_list:\n",
    "        print(Moives_dataSet.iloc[i[0]].title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies: (5209, 4)\n",
      "Vectors: (5211, 8000)\n",
      "Similarity: (5211, 5211)\n",
      "avengers: age of ultron\n",
      "avengers: endgame\n",
      "captain america: the winter soldier\n",
      "captain america: civil war\n",
      "thor\n"
     ]
    }
   ],
   "source": [
    "recommend(\"the avengers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moives_dataSet.to_csv('Movies.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Movies = pd.read_csv('Movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMovie(movie):\n",
    "    try:\n",
    "        url = f\"https://api.themoviedb.org/3/search/movie?api_key={API_KEY}&query={movie}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        if data['results']:\n",
    "            movie_id = data['results'][0]['id']\n",
    "            details_url = f\"https://api.themoviedb.org/3/movie/{movie_id}?api_key={API_KEY}&append_to_response=credits,keywords\"\n",
    "            details = requests.get(details_url).json()\n",
    "            return details\n",
    "        else:\n",
    "            return f\"No search results for {movie}\"\n",
    "    except Exception as e:\n",
    "        return (f\"Error finding the {movie}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def clean_Details(details):\n",
    "    movie_id = details.get('id',None)\n",
    "    title = details.get('title','').lower()\n",
    "    \n",
    "    overview = details.get('overview', '').lower()\n",
    "    genres = [g['name'].lower().replace(\" \", \"\") for g in details.get('genres', [])]\n",
    "    keywords = [k['name'].lower().replace(\" \", \"\") for k in details.get('keywords', {}).get('keywords', [])]\n",
    "    cast = [c['name'].lower().replace(\" \", \"\") for c in details.get('credits', {}).get('cast', [])[:5]]\n",
    "    crew = [c['name'].lower().replace(\" \", \"\") for c in details.get('credits', {}).get('crew', []) if c.get('job') == 'Director']\n",
    "    tagline = details.get('tagline','').lower()\n",
    "\n",
    "    tags = \" \".join(genres + keywords + cast + crew + [overview]+[tagline])\n",
    "    cleaned = pd.DataFrame([{\n",
    "        'id': movie_id,\n",
    "        'title': title,\n",
    "        'tags': tags\n",
    "    }])\n",
    "\n",
    "\n",
    "    def stem(text):\n",
    "        y=[]\n",
    "        for i in text.split():\n",
    "         y.append(ps.stem(i))\n",
    "\n",
    "        return \" \".join(y)\n",
    "\n",
    "    cleaned['tags'] = cleaned['tags'].apply(stem)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfid = TfidfVectorizer(max_features=8000, stop_words='english')\n",
    "vectors = tfid.fit_transform(Movies['tags']).toarray()\n",
    "similarity = cosine_similarity(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movie):\n",
    "    global Movies, similarity,vectors\n",
    "    \n",
    "    movie = movie.lower()\n",
    "\n",
    "    # If not in dataset\n",
    "    if movie not in Movies['title'].values:\n",
    "        details = getMovie(movie)\n",
    "        if not details:\n",
    "            print(f\"No search results for '{movie}'.\")\n",
    "            return\n",
    "        \n",
    "        #details is a dict (valid API response)\n",
    "        if isinstance(details, dict):\n",
    "            new_Movie = clean_Details(details)\n",
    "            new_tags = new_Movie['tags'].iloc[0]\n",
    "\n",
    "            #tranform only the new movie tags and caluclate similarity with existing matrix\n",
    "            new_vector = tfid.transform([new_tags]).toarray()\n",
    "            new_sim = cosine_similarity(new_vector, vectors)[0]\n",
    "            \n",
    "            #append the new movie and vector\n",
    "            Movies = pd.concat([Movies, new_Movie], ignore_index=True)\n",
    "            Movies.drop_duplicates(subset='title', inplace=True)\n",
    "            Movies.reset_index(drop=True, inplace=True)\n",
    "            vectors = np.vstack([vectors, new_vector])\n",
    "\n",
    "            #adding the new sim to the similarity row and column\n",
    "            similarity = np.vstack([similarity, new_sim])\n",
    "            new_col = np.append(new_sim, 1.0).reshape(-1, 1)\n",
    "            similarity = np.hstack([similarity, new_col])\n",
    "\n",
    "            Movies.to_csv(\"movies_data.csv\", index=False)\n",
    "            with open(\"vectors.pkl\", \"wb\") as f:\n",
    "                pickle.dump(vectors, f)\n",
    "            with open(\"similarity.pkl\", \"wb\") as f:\n",
    "                pickle.dump(similarity, f)\n",
    "        else:\n",
    "            print(details)\n",
    "            return\n",
    "        \n",
    "\n",
    "\n",
    "    movie_row = Movies[Movies['title'] == movie]\n",
    "    if movie_row.empty:\n",
    "        print(\"Do you mean\",\"'\",new_Movie['title'].iloc[0],\"'\"\" ?\")\n",
    "        return\n",
    "\n",
    "    movie_index = movie_row.index[0]\n",
    "    distance = similarity[movie_index]\n",
    "    movie_list = sorted(list(enumerate(distance)), reverse=True, key=lambda x: x[1])[1:6]\n",
    "    print(\"Movies:\", Movies.shape)\n",
    "    print(\"Vectors:\", vectors.shape)\n",
    "    print(\"Similarity:\", similarity.shape)\n",
    "    for i in movie_list:\n",
    "        print(Movies.iloc[i[0]].title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies: (5207, 4)\n",
      "Vectors: (5207, 8000)\n",
      "Similarity: (5207, 5207)\n",
      "taken\n",
      "edge of darkness\n",
      "trade of innocents\n",
      "homefront\n",
      "heli\n"
     ]
    }
   ],
   "source": [
    "recommend(\"A working man\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
